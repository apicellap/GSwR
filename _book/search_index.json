[["index.html", "A Guide to Getting Started with R About", " A Guide to Getting Started with R Pete Apicella 2022-07-18 About This resource encompasses my compiled notes on, Getting Started with R: An Introduction for Biologists Second edition. These notes are not meant to replace the textbook. Instead they may act as a helpful guide. "],["getting-acqainted-with-r.html", "Chapter 1 Getting Acqainted with R 1.1 Using R as a giant calculator 1.2 Your first script", " Chapter 1 Getting Acqainted with R 1.1 Using R as a giant calculator Produce a sequence of numbers: seq(from = 0, to = 10, by = 1) #interval ## [1] 0 1 2 3 4 5 6 7 8 9 10 1.2 Your first script Creating objects: x&lt;-seq(from = 0, to = 10, by =0.5) x ## [1] 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0 5.5 6.0 6.5 7.0 ## [16] 7.5 8.0 8.5 9.0 9.5 10.0 y&lt;-seq(from = 101, to = 110, by =0.5) Add the objects together to create a new object: z &lt;- x + y ## Warning in x + y: longer object length is not a multiple of shorter object ## length z ## [1] 101.0 102.0 103.0 104.0 105.0 106.0 107.0 108.0 109.0 110.0 111.0 112.0 ## [13] 113.0 114.0 115.0 116.0 117.0 118.0 119.0 110.5 111.5 Print the session information: sessionInfo() ## R version 4.1.1 (2021-08-10) ## Platform: aarch64-apple-darwin20 (64-bit) ## Running under: macOS Monterey 12.4 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/4.1-arm64/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/4.1-arm64/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## loaded via a namespace (and not attached): ## [1] bookdown_0.27 digest_0.6.29 R6_2.5.1 jsonlite_1.7.3 ## [5] magrittr_2.0.3 evaluate_0.15 stringi_1.7.6 rlang_1.0.2 ## [9] cli_3.2.0 rstudioapi_0.13 jquerylib_0.1.4 bslib_0.3.1 ## [13] rmarkdown_2.14 tools_4.1.1 stringr_1.4.0 xfun_0.31 ## [17] yaml_2.2.1 fastmap_1.1.0 compiler_4.1.1 htmltools_0.5.2 ## [21] knitr_1.39 sass_0.4.0 "],["getting-data-into-r.html", "Chapter 2 Getting Data into R 2.1 Checking that your data are your data 2.2 Appendix advanced activity: dealing with untidy data", " Chapter 2 Getting Data into R install.packages(&quot;dplyr&quot;,repos = &quot;https://cran.us.r-project.org&quot;) install.packages(&quot;tidyr&quot;,repos = &quot;https://cran.us.r-project.org&quot;) install.packages(&quot;stringr&quot;,repos = &quot;https://cran.us.r-project.org&quot;) install.packages(&quot;lubridate&quot;,repos = &quot;https://cran.us.r-project.org&quot;) install.packages(&quot;readr&quot;,repos = &quot;https://cran.us.r-project.org&quot;) library(dplyr) library(tidyr) library(stringr) library(lubridate) library(readr) Read in the data urlfile02a=&quot;https://raw.githubusercontent.com/apicellap/data/main/compensation.csv&quot; compensation&lt;-read.csv(url(urlfile02a)) head(compensation) ## Root Fruit Grazing ## 1 6.225 59.77 Ungrazed ## 2 6.487 60.98 Ungrazed ## 3 4.919 14.73 Ungrazed ## 4 5.130 19.28 Ungrazed ## 5 5.417 34.25 Ungrazed ## 6 5.359 35.53 Ungrazed 2.1 Checking that your data are your data Generate names of the columns/variables in the console: names(compensation) ## [1] &quot;Root&quot; &quot;Fruit&quot; &quot;Grazing&quot; Produce number of observations (rows in each column) followed by # of variables: dim(compensation) ## [1] 40 3 Review structure of the data: str(compensation) ## &#39;data.frame&#39;: 40 obs. of 3 variables: ## $ Root : num 6.22 6.49 4.92 5.13 5.42 ... ## $ Fruit : num 59.8 61 14.7 19.3 34.2 ... ## $ Grazing: chr &quot;Ungrazed&quot; &quot;Ungrazed&quot; &quot;Ungrazed&quot; &quot;Ungrazed&quot; ... 2.2 Appendix advanced activity: dealing with untidy data urlfile02b=&quot;https://raw.githubusercontent.com/apicellap/data/main/nasty%20format.csv&quot; nasty.format&lt;-read.csv(url(urlfile02b)) head(nasty.format) ## Species Bottle Temp X1.2.13 X2.2.13 X3.2.13 X4.2.13 X6.2.13 X8.2.13 ## 1 P.caudatum 7-P.c 22 100.0 58.8 67.5 6.8 0.93 0.39 ## 2 P.caudatum 8-P.c 22 62.5 71.3 67.5 7.9 0.90 0.36 ## 3 P.caudatum 9-P.c 22 75.0 72.5 62.3 7.9 0.88 0.25 ## 4 P.caudatum 22-P.c 20 75.0 73.8 76.3 31.3 3.12 1.01 ## 5 P.caudatum 23-P.c 20 50.0 NA 81.3 32.5 3.75 1.06 ## 6 P.caudatum 24-P.c 20 87.5 NA 62.5 28.8 3.12 1.00 ## X10.2.13 X12.2.13 ## 1 0.19 0.46 ## 2 0.16 0.34 ## 3 0.23 0.31 ## 4 0.56 0.50 ## 5 0.49 0.38 ## 6 0.41 0.46 Review data structure: str(nasty.format) ## &#39;data.frame&#39;: 37 obs. of 11 variables: ## $ Species : chr &quot;P.caudatum&quot; &quot;P.caudatum&quot; &quot;P.caudatum&quot; &quot;P.caudatum&quot; ... ## $ Bottle : chr &quot;7-P.c&quot; &quot;8-P.c&quot; &quot;9-P.c&quot; &quot;22-P.c&quot; ... ## $ Temp : int 22 22 22 20 20 20 15 15 15 22 ... ## $ X1.2.13 : num 100 62.5 75 75 50 87.5 75 50 75 37.5 ... ## $ X2.2.13 : num 58.8 71.3 72.5 73.8 NA NA NA NA NA 52.5 ... ## $ X3.2.13 : num 67.5 67.5 62.3 76.3 81.3 62.5 90 78.8 78.3 23.8 ... ## $ X4.2.13 : num 6.8 7.9 7.9 31.3 32.5 28.8 72.5 92.5 77.5 1.25 ... ## $ X6.2.13 : num 0.93 0.9 0.88 3.12 3.75 ... ## $ X8.2.13 : num 0.39 0.36 0.25 1.01 1.06 1 67.5 72.5 60 0.96 ... ## $ X10.2.13: num 0.19 0.16 0.23 0.56 0.49 0.41 37.5 52.5 60 0.33 ... ## $ X12.2.13: num 0.46 0.34 0.31 0.5 0.38 ... this dataset is poorly constructed Eliminate extra (37th) row in dataset: nasty.format&lt;-filter(nasty.format, Bottle !=&quot;&quot;) # &#39;!=&#39; symbol means &#39;≠&#39; tail(nasty.format) ## Species Bottle Temp X1.2.13 X2.2.13 X3.2.13 X4.2.13 X6.2.13 X8.2.13 ## 31 S. fonticola 19 20 25.0 87.5 85.0 98.8 78.75 71.25 ## 32 S. fonticola 20 20 87.5 63.8 81.3 76.3 72.50 85.00 ## 33 S. fonticola 21 20 50.0 77.5 83.8 97.5 68.75 71.25 ## 34 S. fonticola 34 15 50.0 NA 101.3 93.8 70.00 91.25 ## 35 S. fonticola 35 15 62.5 NA 65.0 72.5 61.25 72.50 ## 36 S. fonticola 36 15 112.5 NA 76.3 67.5 61.25 77.50 ## X10.2.13 X12.2.13 ## 31 68.8 101.25 ## 32 72.5 85.00 ## 33 60.0 98.75 ## 34 76.3 80.00 ## 35 66.3 102.50 ## 36 91.3 77.50 this filter function is programmed to capture every row in which variable, ‘Bottle’, contains text Create new variables and assort data into them: tidy_data &lt;- gather(nasty.format, Date, Abundance, #the variables to be created 4:11) #column headers that are dates in the nasty.format dataframe head(tidy_data) ## Species Bottle Temp Date Abundance ## 1 P.caudatum 7-P.c 22 X1.2.13 100.0 ## 2 P.caudatum 8-P.c 22 X1.2.13 62.5 ## 3 P.caudatum 9-P.c 22 X1.2.13 75.0 ## 4 P.caudatum 22-P.c 20 X1.2.13 75.0 ## 5 P.caudatum 23-P.c 20 X1.2.13 50.0 ## 6 P.caudatum 24-P.c 20 X1.2.13 87.5 Remove the ‘X’, which precedes that date in each observation: tidy_data &lt;- mutate(tidy_data, Date=substr(Date,2,20)) head(tidy_data) ## Species Bottle Temp Date Abundance ## 1 P.caudatum 7-P.c 22 1.2.13 100.0 ## 2 P.caudatum 8-P.c 22 1.2.13 62.5 ## 3 P.caudatum 9-P.c 22 1.2.13 75.0 ## 4 P.caudatum 22-P.c 20 1.2.13 75.0 ## 5 P.caudatum 23-P.c 20 1.2.13 50.0 ## 6 P.caudatum 24-P.c 20 1.2.13 87.5 Display all unique dates: unique( tidy_data$Date) #this says use the observations in the variable &#39;Date&#39; in the &#39;tidy_data&#39; dataframe ## [1] &quot;1.2.13&quot; &quot;2.2.13&quot; &quot;3.2.13&quot; &quot;4.2.13&quot; &quot;6.2.13&quot; &quot;8.2.13&quot; &quot;10.2.13&quot; ## [8] &quot;12.2.13&quot; Reformat the dates to be universally recognized: tidy_data &lt;-mutate(tidy_data, Date=dmy(Date)) head(tidy_data) ## Species Bottle Temp Date Abundance ## 1 P.caudatum 7-P.c 22 2013-02-01 100.0 ## 2 P.caudatum 8-P.c 22 2013-02-01 62.5 ## 3 P.caudatum 9-P.c 22 2013-02-01 75.0 ## 4 P.caudatum 22-P.c 20 2013-02-01 75.0 ## 5 P.caudatum 23-P.c 20 2013-02-01 50.0 ## 6 P.caudatum 24-P.c 20 2013-02-01 87.5 separate() Separates information present in one column to multiple new columns unite() Puts information from several columns into one column rbind() Puts datasets with exactly the same columns together cbind() Combines two datasets with exactly the same columns together full_join() Joins two datasets with one or more columns in common merge() Same function as full_join() but from the base package "],["data-management-and-manipulation.html", "Chapter 3 Data Management and Manipulation 3.1 Subsetting 3.2 Calculating summary statistics about groups of your data", " Chapter 3 Data Management and Manipulation install.packages(&quot;dplyr&quot;,repos = &quot;https://cran.us.r-project.org&quot;) install.packages(&quot;tidyr&quot;,repos = &quot;https://cran.us.r-project.org&quot;) install.packages(&quot;stringr&quot;,repos = &quot;https://cran.us.r-project.org&quot;) install.packages(&quot;lubridate&quot;,repos = &quot;https://cran.us.r-project.org&quot;) library(dplyr) library(tidyr) library(stringr) library(lubridate) Read in the data urlfile03a=&quot;https://raw.githubusercontent.com/apicellap/data/main/compensation.csv&quot; compensation&lt;-read.csv(url(urlfile03a)) head(compensation) ## Root Fruit Grazing ## 1 6.225 59.77 Ungrazed ## 2 6.487 60.98 Ungrazed ## 3 4.919 14.73 Ungrazed ## 4 5.130 19.28 Ungrazed ## 5 5.417 34.25 Ungrazed ## 6 5.359 35.53 Ungrazed Summarize the data in each variable: summary(compensation) ## Root Fruit Grazing ## Min. : 4.426 Min. : 14.73 Length:40 ## 1st Qu.: 6.083 1st Qu.: 41.15 Class :character ## Median : 7.123 Median : 60.88 Mode :character ## Mean : 7.181 Mean : 59.41 ## 3rd Qu.: 8.510 3rd Qu.: 76.19 ## Max. :10.253 Max. :116.05 3.1 Subsetting Create new dataframe comprised of specific variable(s): head(select(compensation, Fruit)) ## Fruit ## 1 59.77 ## 2 60.98 ## 3 14.73 ## 4 19.28 ## 5 34.25 ## 6 35.53 Select all columns except one: head(select(compensation, -Root)) ## Fruit Grazing ## 1 59.77 Ungrazed ## 2 60.98 Ungrazed ## 3 14.73 Ungrazed ## 4 19.28 Ungrazed ## 5 34.25 Ungrazed ## 6 35.53 Ungrazed Create new dataframe comrpised of specific variable(s) except ‘Root’: head(slice(compensation, 2:10)) ## Root Fruit Grazing ## 1 6.487 60.98 Ungrazed ## 2 4.919 14.73 Ungrazed ## 3 5.130 19.28 Ungrazed ## 4 5.417 34.25 Ungrazed ## 5 5.359 35.53 Ungrazed ## 6 7.614 87.73 Ungrazed Create new dataframe comprised of a list of variables: head(slice(compensation, c(2,3,10))) ## Root Fruit Grazing ## 1 6.487 60.98 Ungrazed ## 2 4.919 14.73 Ungrazed ## 3 6.930 64.34 Ungrazed Filter data set to only observations in which this is TRUE: filter(compensation, Fruit == 80) ## [1] Root Fruit Grazing ## &lt;0 rows&gt; (or 0-length row.names) Grab observations when Fruit is not equal to 80: head(filter(compensation, Fruit !=80)) ## Root Fruit Grazing ## 1 6.225 59.77 Ungrazed ## 2 6.487 60.98 Ungrazed ## 3 4.919 14.73 Ungrazed ## 4 5.130 19.28 Ungrazed ## 5 5.417 34.25 Ungrazed ## 6 5.359 35.53 Ungrazed Grab any observations in which Fruit is ≤ 80; can also use &lt; symbol for less than: head(filter(compensation, Fruit &lt;=80)) ## Root Fruit Grazing ## 1 6.225 59.77 Ungrazed ## 2 6.487 60.98 Ungrazed ## 3 4.919 14.73 Ungrazed ## 4 5.130 19.28 Ungrazed ## 5 5.417 34.25 Ungrazed ## 6 5.359 35.53 Ungrazed Grab any observations in which Fruit is greater than 95 OR less than 15: head(filter(compensation, Fruit &gt;95|Fruit&lt;15)) ## Root Fruit Grazing ## 1 4.919 14.73 Ungrazed ## 2 10.253 116.05 Grazed ## 3 6.106 14.95 Grazed ## 4 9.844 105.07 Grazed ## 5 9.351 98.47 Grazed Grab any observations in which Fruit is greater than 50 AND less than 55: head(filter(compensation, Fruit &gt;50 &amp; Fruit&lt;55)) ## Root Fruit Grazing ## 1 6.248 52.92 Ungrazed ## 2 6.013 53.61 Ungrazed ## 3 5.928 54.86 Ungrazed ## 4 7.354 50.08 Grazed ## 5 8.158 52.26 Grazed Order data by Fruit from lowest to highest observation: head(arrange(compensation, Fruit)) ## Root Fruit Grazing ## 1 4.919 14.73 Ungrazed ## 2 6.106 14.95 Grazed ## 3 4.426 18.89 Ungrazed ## 4 5.130 19.28 Ungrazed ## 5 4.975 24.25 Ungrazed ## 6 5.451 32.35 Ungrazed Create new dataframe that filters observations that have Fruit values above 80 and only contains the corresponding Root values: head(select(filter(compensation, Fruit&gt;80), Root)) ## Root ## 1 7.614 ## 2 7.001 ## 3 10.253 ## 4 9.039 ## 5 8.988 ## 6 8.975 3.2 Calculating summary statistics about groups of your data Perform summary analyses on dataframe: summarise( group_by(compensation, Grazing), #access the dataframe, target Grazing to be the grouping variable meanFruit = mean(Fruit)) #creates the object, meanFruit which is the mean of the data in the Fruit variable ## # A tibble: 2 × 2 ## Grazing meanFruit ## &lt;chr&gt; &lt;dbl&gt; ## 1 Grazed 67.9 ## 2 Ungrazed 50.9 Additional summary functions and create new dataframe to encompass calculations: mean.fruit&lt;-summarise( group_by(compensation, Grazing), meanFruit = mean(Fruit), sdfruit =sd(Fruit)) #multiple statistics can be calculated within summarise mean.fruit ## # A tibble: 2 × 3 ## Grazing meanFruit sdfruit ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Grazed 67.9 25.0 ## 2 Ungrazed 50.9 21.8 x &lt;- sum(with(compensation, Grazing == &quot;Grazed&quot;)) #counts number of observations for variable when it = Grazed x ## [1] 20 SE.mean.fruit&lt;-summarise( group_by(compensation, Grazing), meanFruit = mean(Fruit), SEfruit =(sd(Fruit))/sqrt(x)) #multiple statistics can be calculated within summarise SE.mean.fruit ## # A tibble: 2 × 3 ## Grazing meanFruit SEfruit ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Grazed 67.9 5.58 ## 2 Ungrazed 50.9 4.87 "],["visualizing-your-data.html", "Chapter 4 Visualizing your data 4.1 The first step in every data analysis - making a picture 4.2 ggplot2: a grammar for graphics 4.3 Box and whisker plots 4.4 Distributions: making histograms of numeric variables", " Chapter 4 Visualizing your data 4.1 The first step in every data analysis - making a picture install.packages(&quot;ggplot2&quot;, repos = &quot;https://cran.us.r-project.org&quot;) install.packages(&quot;dplyr&quot;, repos = &quot;https://cran.us.r-project.org&quot;) library(ggplot2) library(dplyr) Read in the data urlfile04a=&quot;https://raw.githubusercontent.com/apicellap/data/main/compensation.csv&quot; compensation&lt;-read.csv(url(urlfile04a)) View dataframe &amp; read the variables + the first few of their observations horizontally: glimpse(compensation) ## Rows: 40 ## Columns: 3 ## $ Root &lt;dbl&gt; 6.225, 6.487, 4.919, 5.130, 5.417, 5.359, 7.614, 6.352, 4.975,… ## $ Fruit &lt;dbl&gt; 59.77, 60.98, 14.73, 19.28, 34.25, 35.53, 87.73, 63.21, 24.25,… ## $ Grazing &lt;chr&gt; &quot;Ungrazed&quot;, &quot;Ungrazed&quot;, &quot;Ungrazed&quot;, &quot;Ungrazed&quot;, &quot;Ungrazed&quot;, &quot;U… 4.2 ggplot2: a grammar for graphics Create base plot: base_plot &lt;-ggplot(compensation, aes(x = Root, y = Fruit, colour=Grazing)) + #colour: for the two levels of the categorical variable, Grazing geom_point() base_plot Render background white instead of gray: base_plot + theme_bw() base_plot + theme_bw() + geom_point( size = 5) #alter size of datapoints in scatterplot Add x and y axis titles: base_plot + theme_bw() + geom_point(size = 5) + xlab(&quot;Root Biomass&quot;) + ylab(&quot;Fruit Production&quot;) 4.3 Box and whisker plots base_plot2 &lt;- ggplot(compensation, aes(x = Grazing, y = Fruit)) + geom_boxplot() + geom_point( size = 4, #size of point colour = &#39;lightgrey&#39;, #color of point alpha = 0.5) + #transparency of point xlab(&quot;Grazing treatment&quot;) + ylab(&quot;Fruit Production&quot;) + theme_bw() base_plot2 4.4 Distributions: making histograms of numeric variables ggplot(compensation, aes(x=Fruit))+ geom_histogram(bins=15) #bins defines how many histogram bins there are ggplot(compensation, aes(x=Fruit))+ geom_histogram(bins=15) + facet_wrap(~Grazing) #facet_wrap() allows you to put the plots next to each other, a variable must be specified "],["introduction-to-statistics-in-r.html", "Chapter 5 Introduction to Statistics in R 5.1 \\(\\chi\\)\\(^{2}\\) contingency table analysis 5.2 Making the \\(\\chi\\)\\(^{2}\\) Test 5.3 Two-sample t-test 5.4 Linear models 5.5 Simple linear regression 5.6 Analysis of variance: the one-way ANOVA", " Chapter 5 Introduction to Statistics in R install.packages(&quot;ggplot2&quot;, repos = &quot;https://cran.us.r-project.org&quot;) install.packages(&quot;dplyr&quot;, repos = &quot;https://cran.us.r-project.org&quot;) install.packages(&quot;ggfortify&quot;, repos = &quot;https://cran.us.r-project.org&quot;) library(ggplot2) library(dplyr) library(ggfortify) urlfile05a=&quot;https://raw.githubusercontent.com/apicellap/data/main/ladybirds_morph_colour.csv&quot; lady&lt;-read.csv(url(urlfile05a)) head(lady) ## Habitat Site morph_colour number ## 1 Rural R1 black 10 ## 2 Rural R2 black 3 ## 3 Rural R3 black 4 ## 4 Rural R4 black 7 ## 5 Rural R5 black 6 ## 6 Rural R1 red 15 View data structure: str(lady) ## &#39;data.frame&#39;: 20 obs. of 4 variables: ## $ Habitat : chr &quot;Rural&quot; &quot;Rural&quot; &quot;Rural&quot; &quot;Rural&quot; ... ## $ Site : chr &quot;R1&quot; &quot;R2&quot; &quot;R3&quot; &quot;R4&quot; ... ## $ morph_colour: chr &quot;black&quot; &quot;black&quot; &quot;black&quot; &quot;black&quot; ... ## $ number : int 10 3 4 7 6 15 18 9 12 16 ... 5.1 \\(\\chi\\)\\(^{2}\\) contingency table analysis Create new dataframe, totals: totals &lt;- lady %&gt;% #working with this dataframe group_by(Habitat, morph_colour) %&gt;% #we want to represent these groups in the dataframe summarise(total.number = sum(number)) #add up the numbers of each group using the new object total.number ## `summarise()` has grouped output by &#39;Habitat&#39;. You can override using the ## `.groups` argument. totals ## # A tibble: 4 × 3 ## # Groups: Habitat [2] ## Habitat morph_colour total.number ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 Industrial black 115 ## 2 Industrial red 85 ## 3 Rural black 30 ## 4 Rural red 70 Create bar chart: base_plot&lt;-ggplot(totals, aes(x=Habitat, y=total.number, fill = morph_colour)) + #fill is used when there is something like a bar that can be filled with color #if this were color = morph_colour, then the argument would affect the bar&#39;s outline geom_bar( stat = &#39;identity&#39;, #this tells ggplot not to calculate anything from the data and just display the data as they are in the dataframe position = &#39;dodge&#39; #this is request to put the two bars in each Habitat group next to each other #if it&#39;s not used, a stacked barplot would be printed ) base_plot base_plot + scale_fill_manual(values = c(black = &quot;black&quot;, red = &quot;red&quot;)) #the text in &quot;&quot; are the colors we are instructing R to fill the bars with Null hypothesis: there is no association between the color of the birds and their habitat My opinion: data suggest that there is a higher proportion of colored birds in the industrial habitat. We should reject the null hypothesis (pre-stats) to rigorously test this, a chi-squared test must be performed 5.2 Making the \\(\\chi\\)\\(^{2}\\) Test Use xtabs() function to generate a contingency table: lady.mat &lt;-xtabs( #function converts dataframe into a matrix, which is different from a dataframe number ~ Habitat + morph_colour, #cross-tabulate the number of column counts in the dataframe by the Habitat and morph_colour variables data = lady) lady.mat ## morph_colour ## Habitat black red ## Industrial 115 85 ## Rural 30 70 Perform a \\(\\chi\\)\\(^{2}\\) test on the matrix: lady.chi&lt;-chisq.test(lady.mat) #chi squared test function performed on matrix lady.chi ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: lady.mat ## X-squared = 19.103, df = 1, p-value = 1.239e-05 the p value is 0.00001239 - this is the probability that the pattern arose by chance. It is lower than 0.05, so we can reject the null hypothesis names(lady.chi) #can examine all of the parts of the test mechanics ## [1] &quot;statistic&quot; &quot;parameter&quot; &quot;p.value&quot; &quot;method&quot; &quot;data.name&quot; &quot;observed&quot; ## [7] &quot;expected&quot; &quot;residuals&quot; &quot;stdres&quot; 5.3 Two-sample t-test The two-sample t-test compares the means of two groups of numeric values It is appropriate when the sample sizes of these two groups is small The analysis makes the following assumptions about the data: The data are normally distributed The variances of the data are equivalent urlfile05b=&quot;https://raw.githubusercontent.com/apicellap/data/main/ozone.csv&quot; ozone&lt;-read.csv(url(urlfile05b)) glimpse(ozone) ## Rows: 20 ## Columns: 3 ## $ Ozone &lt;dbl&gt; 61.7, 64.0, 72.4, 56.8, 52.4, 44.8, 70.4, 67.6, 68.8, … ## $ Garden.location &lt;chr&gt; &quot;West&quot;, &quot;West&quot;, &quot;West&quot;, &quot;West&quot;, &quot;West&quot;, &quot;West&quot;, &quot;West&quot;… ## $ Garden.ID &lt;chr&gt; &quot;G1&quot;, &quot;G2&quot;, &quot;G3&quot;, &quot;G4&quot;, &quot;G5&quot;, &quot;G6&quot;, &quot;G7&quot;, &quot;G8&quot;, &quot;G9&quot;, … 5.3.1 The first step: Plot your data Create histograms of the data, by Garden.location variable: ggplot(ozone, aes(x=Ozone))+ #since this is a histogram, x must be a continuous variable; cannot be categorical geom_histogram(binwidth=10) + facet_wrap(~Garden.location, #Divide data up into groups by this variable ncol=1 ) + #stacks the graphs on top of each other - 1 column theme_bw() graph shows that assumptions of normality and equality of variance are met R has functions to evaluate these aspects more rigorously too Count the number of observations when variable is ‘East’: x&lt;-sum(with(ozone, Garden.location == &quot;East&quot;)) x ## [1] 10 summary&lt;-ozone %&gt;% group_by(Garden.location) %&gt;% summarise(mean = mean(Ozone), SE = (sd(Ozone))/sqrt(x)) summary ## # A tibble: 2 × 3 ## Garden.location mean SE ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 East 77.3 2.49 ## 2 West 61.3 2.87 Plot the ozone data as a barplot: OzBrP&lt;-ggplot(summary, aes(x = Garden.location, y = mean)) + #x defined as grazing treatment, y as fruit production geom_col() + geom_errorbar(aes(x=Garden.location, ymin=mean -SE, ymax=mean*1+SE, width=.2), position = &#39;dodge&#39;) OzBrP Plot the ozone data as a boxplot: OzBxP &lt;-ggplot(ozone, aes(x = Garden.location, y = Ozone)) + geom_boxplot() OzBxP null hypothesis: there is no difference in ozone levels between the East and West garden locations my opinion based on the boxplot - reject the null hypothesis 5.3.2 Two sample t-test analysis Perform two sample t-test on the ozone dataset: t.test(Ozone ~ Garden.location, #expression means do ozone levels vary as a function of location? That&#39;s how the ~ reads data = ozone) ## ## Welch Two Sample t-test ## ## data: Ozone by Garden.location ## t = 4.2363, df = 17.656, p-value = 0.0005159 ## alternative hypothesis: true difference in means between group East and group West is not equal to 0 ## 95 percent confidence interval: ## 8.094171 24.065829 ## sample estimates: ## mean in group East mean in group West ## 77.34 61.26 the default of this two sample t-test is the Welch’s version In Welch version, the assumption about equal variance is relaxed and allows that you do not have to test for equal variance The GSwR authors suggest not testing for equal variance note regarding the Confidence interval (CI): the line about the 95% CI is 8.1-24.1. Since this range does not include 0 says that the means are statistically different from each other. This is congruent with the p value of the t-test 5.4 Linear models Linear models are a class of analyses that include regression, multiple regression, ANOVA, and ANCOVA they are centered around a similiar framework of ideas such as a common set of assumptions about the data like the idea that the data are normally distributed 5.5 Simple linear regression This section focuses on a dataset that compares plant growth rates to soil moisture content The response (dependent) variable is that of plant growth rate Plant growth rate is plotted against the explanatory (independent) variable, soil moisture content This variable is a continuous, numeric variable which does not have categories Read and view structure of the data: urlfile05c=&quot;https://raw.githubusercontent.com/apicellap/data/main/plant.growth.rate.csv&quot; plant_gr&lt;-read.csv(url(urlfile05c)) glimpse(plant_gr) #two continuous (numeric) variables. they have no categories ## Rows: 50 ## Columns: 2 ## $ soil.moisture.content &lt;dbl&gt; 0.4696876, 0.5413106, 1.6979915, 0.8255799, 0.85… ## $ plant.growth.rate &lt;dbl&gt; 21.31695, 27.03072, 38.98937, 30.19529, 37.06547… 5.5.1 Getting and plotting the data Plot the plant growth (pg) data: ggplot(plant_gr, aes(x=soil.moisture.content, y=plant.growth.rate)) + geom_point()+ ylab(&quot;Plant Growth Rate (mm/week)&quot;) + theme_bw() Plot shows that there is a probably a positive linear relationship between the two variables There is a slope of about 15 mm/week preliminary analysis: probably will reject the null hypothesis that soil moisture does not affect plant growth 5.5.2 Making a simple linear regression happen Create a linear model: model_pgr &lt;- lm(plant.growth.rate ~ soil.moisture.content, data = plant_gr) #plant growth rate is a function of soil moisture content model_pgr ## ## Call: ## lm(formula = plant.growth.rate ~ soil.moisture.content, data = plant_gr) ## ## Coefficients: ## (Intercept) soil.moisture.content ## 19.35 12.75 5.5.3 Assumptions first Check assumptions with ggfortify: autoplot(model_pgr, smooth.colour = NA) ggfortify plots: Top left - informs us on whether the line is an appropriate fit to the data; flat line means model is a good fit. Humps/valleys -&gt; poor fit Top right - dots = residuals; dashes line = expectation under normal distribution. Better tool than histogram to assess normal distribution Bottom left - evaluates assumption of equal variance. linear models assume that variance is constant over all predicted values of the response variable. There should be no pattern Bottom right - evaluates leverage. Used to detect influential datapoints that will shift the gradient more than expected + also for outliers null hypothesis: soil moisture has no effect on plant growth 5.5.4 Now the interpretation Produce a sum of squares table: anova(model_pgr) ## Analysis of Variance Table ## ## Response: plant.growth.rate ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## soil.moisture.content 1 2521.15 2521.15 156.08 &lt; 2.2e-16 *** ## Residuals 48 775.35 16.15 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Output: Large F value indicates that error variance is small relative to the variance attributed to the explanatory variable This leads to the tiny p value. Both are good indications that the effect seen in the data isn’t the result of chance Produce a summary table: summary(model_pgr) ## ## Call: ## lm(formula = plant.growth.rate ~ soil.moisture.content, data = plant_gr) ## ## Residuals: ## Min 1Q Median 3Q Max ## -8.9089 -3.0747 0.2261 2.6567 8.9406 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 19.348 1.283 15.08 &lt;2e-16 *** ## soil.moisture.content 12.750 1.021 12.49 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.019 on 48 degrees of freedom ## Multiple R-squared: 0.7648, Adjusted R-squared: 0.7599 ## F-statistic: 156.1 on 1 and 48 DF, p-value: &lt; 2.2e-16 produces table of estimates of the coefficients of the line that is the model the slope that is associated with the explanatory variable (soil moisture) - the values of which are associated with the differences in plant growth rate Superimpose linear model onto plot: ggplot(plant_gr, aes(x=soil.moisture.content, y=plant.growth.rate)) + geom_point()+ geom_smooth(method = &#39;lm&#39;) + #put a linear-model fitted line and the standard error of the fit using flash transparent gray onto graph ylab(&quot;Plant Growth Rate (mm/week)&quot;) + theme_bw() ## `geom_smooth()` using formula &#39;y ~ x&#39; 5.6 Analysis of variance: the one-way ANOVA the one-way ANOVA is similiar to the previous example and the two-sample t-test with one key difference: the explanatory variable in the one-way ANOVA is a categorical variable (factor) waterflea dataset - we ask: whether parasites alter waterflea growth rates whether each of three parasittes reduces growth rates relative to a control in which there was no parasite urlfile05d=&quot;https://raw.githubusercontent.com/apicellap/data/main/Daphniagrowth.csv&quot; daphnia&lt;-read.csv(url(urlfile05d)) glimpse(daphnia) ## Rows: 40 ## Columns: 3 ## $ parasite &lt;chr&gt; &quot;control&quot;, &quot;control&quot;, &quot;control&quot;, &quot;control&quot;, &quot;control&quot;, &quot;co… ## $ rep &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, … ## $ growth.rate &lt;dbl&gt; 1.0747092, 1.2659016, 1.3151563, 1.0757519, 1.1967619, 1.3… Plot daphnia data: ggplot(daphnia, aes(x = parasite, y=growth.rate)) + geom_boxplot() + theme_bw() + coord_flip() #flips the orientation of the graph 90˚ to the right Visualization takeaways: There is substantial variation among the parasite groups The control group has the highest growth rate There is likely to be an overall effect of parasites on growth rate There may be an order in the impacts different parasites have on growth rates: P. ramosa &lt; M. bicuspidata &lt; P. perplexa Create a model: model_grow &lt;- lm(growth.rate ~ parasite, data = daphnia) model_grow ## ## Call: ## lm(formula = growth.rate ~ parasite, data = daphnia) ## ## Coefficients: ## (Intercept) parasiteMetschnikowia bicuspidata ## 1.2139 -0.4128 ## parasitePansporella perplexa parasitePasteuria ramosa ## -0.1376 -0.7317 Check the assumptions: autoplot(model_grow, smooth.colour = NA) assumption-checking plots: The figures suggest that the assumptions are fine Even the upper right plot, the Q-Q plot, is within the bounds of expected variation Produce anova table for model: anova(model_grow) ## Analysis of Variance Table ## ## Response: growth.rate ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## parasite 3 3.1379 1.04597 32.325 2.571e-10 *** ## Residuals 36 1.1649 0.03236 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 For a one-way anova, the null hypothesis is that all of the groups come from populations with (statistically) the same mean The F-value quantifies how likely this is to be true F value is the ratio between the between group variation: within group variation A large F value means that the between group variation is much larger summary(model_grow) ## ## Call: ## lm(formula = growth.rate ~ parasite, data = daphnia) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.41930 -0.09696 0.01408 0.12267 0.31790 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.21391 0.05688 21.340 &lt; 2e-16 *** ## parasiteMetschnikowia bicuspidata -0.41275 0.08045 -5.131 1.01e-05 *** ## parasitePansporella perplexa -0.13755 0.08045 -1.710 0.0959 . ## parasitePasteuria ramosa -0.73171 0.08045 -9.096 7.34e-11 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.1799 on 36 degrees of freedom ## Multiple R-squared: 0.7293, Adjusted R-squared: 0.7067 ## F-statistic: 32.33 on 3 and 36 DF, p-value: 2.571e-10 Output: table of coefficients: What is labelled as ‘(Intercept)’ is the control R tends to list things in alphabetical order Of the levels of the explanatory variable, ‘control’ is the first alphabetically In this context, we can assume that (Intercept) refers to the first level in alphabetical order - control - in this case Treatment contrasts report the differences between the reference level (the control in this case) and the other levels So -0.41275 is the difference between the control level and the parasiteMetschnikowia bicuspidata level In other words, the differences are the distances between the colored diamonds and the dotted black line in figure 6.1 below Determine the means for each level: sumDat &lt;- daphnia %&gt;% group_by(parasite) %&gt;% summarise(meanGR = mean(growth.rate)) sumDat ## # A tibble: 4 × 2 ## parasite meanGR ## &lt;chr&gt; &lt;dbl&gt; ## 1 control 1.21 ## 2 Metschnikowia bicuspidata 0.801 ## 3 Pansporella perplexa 1.08 ## 4 Pasteuria ramosa 0.482 Manually determine the difference between control and other levels: 0.8011541 -1.2139088 ## [1] -0.4127547 1.0763551 - 1.2139088 ## [1] -0.1375537 0.4822030 - 1.2139088 ## [1] -0.7317058 ggplot(daphnia, aes(x = parasite, y=growth.rate, color = parasite)) + geom_point(size =2) + geom_point(data = sumDat, aes( x = parasite, y = meanGR), shape = 18, size = 5) + geom_hline(yintercept = sumDat$meanGR[1], lwd=1, linetype = &#39;dotted&#39;, colour=&quot;black&quot;) + geom_hline(yintercept = sumDat$meanGR[1], lwd=1, linetype = &#39;dotted&#39;, colour=&quot;black&quot;) + xlab(&quot;&quot;)+ theme_bw() + coord_flip() Figure 5.1: Daphnia Treatment Differences "],["advancing-statistics-in-r.html", "Chapter 6 Advancing Statistics in R 6.1 The two-way ANOVA 6.2 Analysis of Covariance (ANCOVA)", " Chapter 6 Advancing Statistics in R install.packages(&quot;ggplot2&quot;, repos = &quot;https://cran.us.r-project.org&quot;) install.packages(&quot;dplyr&quot;, repos = &quot;https://cran.us.r-project.org&quot;) install.packages(&quot;ggfortify&quot;, repos = &quot;https://cran.us.r-project.org&quot;) library(ggplot2) library(dplyr) library(ggfortify) 6.1 The two-way ANOVA the two-way ANOVA differs from the one-way version in that it involves two categorical/explanatory variables the response variable in the two-way ANOVA may vary with both explanatory variables if the way it varies depends on the other explanatory variable, then this phenomenon is known as a statistical interaction Read in data: urlfile06a=&quot;https://raw.githubusercontent.com/apicellap/data/main/growth.csv&quot; growth.moo&lt;-read.csv(url(urlfile06a)) glimpse(growth.moo) ## Rows: 48 ## Columns: 3 ## $ supplement &lt;chr&gt; &quot;supergain&quot;, &quot;supergain&quot;, &quot;supergain&quot;, &quot;supergain&quot;, &quot;contro… ## $ diet &lt;chr&gt; &quot;wheat&quot;, &quot;wheat&quot;, &quot;wheat&quot;, &quot;wheat&quot;, &quot;wheat&quot;, &quot;wheat&quot;, &quot;whea… ## $ gain &lt;dbl&gt; 17.37125, 16.81489, 18.08184, 15.78175, 17.70656, 18.22717,… 6.1.1 Cow Growth data this dataset deals with cows on different diets cows were fed one of three diets: wheat, oats, or barley each diet was then enhanced with one of four supplements: supergrain, control, supersupp, agrimore each diet combination had 3 biological replicates (cows) Coerce two of the variables into factors: growth.moo$supplement &lt;-as.factor(growth.moo$supplement) growth.moo$diet &lt;-as.factor(growth.moo$diet) levels(growth.moo$diet) ## [1] &quot;barley&quot; &quot;oats&quot; &quot;wheat&quot; levels(growth.moo$supplement) ## [1] &quot;agrimore&quot; &quot;control&quot; &quot;supergain&quot; &quot;supersupp&quot; in both of the previous code chunks the output is in alphabetical order R always declares the first level alphabetically the reference level in the case of the supplement variable, control must be made the reference level this can be accomplished using the relevel() function: growth.moo &lt;- mutate(growth.moo, supplement = relevel(supplement, ref = &quot;control&quot;)) levels(growth.moo$supplement) ## [1] &quot;control&quot; &quot;agrimore&quot; &quot;supergain&quot; &quot;supersupp&quot; Create summary dataframe: sumMoo &lt;- growth.moo %&gt;% group_by(diet,supplement) %&gt;% summarise(meanGrow = mean(gain)) ## `summarise()` has grouped output by &#39;diet&#39;. You can override using the ## `.groups` argument. sumMoo ## # A tibble: 12 × 3 ## # Groups: diet [3] ## diet supplement meanGrow ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 barley control 23.3 ## 2 barley agrimore 26.3 ## 3 barley supergain 22.5 ## 4 barley supersupp 25.6 ## 5 oats control 20.5 ## 6 oats agrimore 23.3 ## 7 oats supergain 19.7 ## 8 oats supersupp 21.9 ## 9 wheat control 17.4 ## 10 wheat agrimore 19.6 ## 11 wheat supergain 17.0 ## 12 wheat supersupp 19.7 ggplot(sumMoo, aes(x=supplement, y=meanGrow, colour = diet, #adds color by group group = diet)) + # geom_point() + geom_line() + #connects the dots in each dataset theme_bw() the different supplements might have growth stimulating effects the barley and/or oats diets might be better in stimulating growth relative to the wheat diet since the lines are about parallel with each other, then we can probably rule out interaction effects. This means the effect of supplement probably does not depend on the effect of diet but there might be an additive effect null hypothesis: the effect of supplement type does not depend on diet Create a model to evaluate the null hypothesis: model_cow &lt;- lm(gain ~ diet * supplement, data=growth.moo) model_cow ## ## Call: ## lm(formula = gain ~ diet * supplement, data = growth.moo) ## ## Coefficients: ## (Intercept) dietoats ## 23.2966499 -2.8029851 ## dietwheat supplementagrimore ## -5.8911317 3.0518277 ## supplementsupergain supplementsupersupp ## -0.8305263 2.2786527 ## dietoats:supplementagrimore dietwheat:supplementagrimore ## -0.2471088 -0.8182729 ## dietoats:supplementsupergain dietwheat:supplementsupergain ## -0.0001351 0.4374395 ## dietoats:supplementsupersupp dietwheat:supplementsupersupp ## -0.9120830 -0.0158299 function asks if the growth depends on diet, supplement, or an interaction between diet:supplement Check assumptions: autoplot(model_cow, smooth.colour = NA) plot output: they look okay, but not ideal top left - no pattern that would suggest model is inappropriate top right - less than ideal because the positive and negative residuals are smaller than expected but the general linear model will be able to deal with these deviations from normality bottom left - looks fine, virtually no pattern bottom right - no serious outliers, looks fine Create ANOVA table: anova(model_cow) ## Analysis of Variance Table ## ## Response: gain ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## diet 2 287.171 143.586 83.5201 2.999e-14 *** ## supplement 3 91.881 30.627 17.8150 2.952e-07 *** ## diet:supplement 6 3.406 0.568 0.3302 0.9166 ## Residuals 36 61.890 1.719 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 the diet:supplement row shows a small F value and a p value of 0.91 (way bigger than 0.05) there is no statistically significant effect contributed by the two variables together revisit the importance of a big F value Create summary table: summary(model_cow) ## ## Call: ## lm(formula = gain ~ diet * supplement, data = growth.moo) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.48756 -1.00368 -0.07452 1.03496 2.68069 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 23.2966499 0.6555863 35.536 &lt; 2e-16 *** ## dietoats -2.8029851 0.9271390 -3.023 0.00459 ** ## dietwheat -5.8911317 0.9271390 -6.354 2.34e-07 *** ## supplementagrimore 3.0518277 0.9271390 3.292 0.00224 ** ## supplementsupergain -0.8305263 0.9271390 -0.896 0.37631 ## supplementsupersupp 2.2786527 0.9271390 2.458 0.01893 * ## dietoats:supplementagrimore -0.2471088 1.3111726 -0.188 0.85157 ## dietwheat:supplementagrimore -0.8182729 1.3111726 -0.624 0.53651 ## dietoats:supplementsupergain -0.0001351 1.3111726 0.000 0.99992 ## dietwheat:supplementsupergain 0.4374395 1.3111726 0.334 0.74060 ## dietoats:supplementsupersupp -0.9120830 1.3111726 -0.696 0.49113 ## dietwheat:supplementsupersupp -0.0158299 1.3111726 -0.012 0.99043 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.311 on 36 degrees of freedom ## Multiple R-squared: 0.8607, Adjusted R-squared: 0.8182 ## F-statistic: 20.22 on 11 and 36 DF, p-value: 3.295e-12 output: the intercept is the reference point for the table. it’s the barley-control because barley is alphabetically first and we already set the reference to be the control the Estimate column is the difference between the reference level and the level in each row Generate new summary table: sumMoo &lt;- growth.moo %&gt;% group_by(diet,supplement) %&gt;% summarise(meanGrow = mean(gain), seGrow = sd(gain)/ sqrt( n())) #n() counts the number of observations in each group ## `summarise()` has grouped output by &#39;diet&#39;. You can override using the ## `.groups` argument. #side note: n() will count observations even if they have missing values sumMoo ## # A tibble: 12 × 4 ## # Groups: diet [3] ## diet supplement meanGrow seGrow ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 barley control 23.3 0.703 ## 2 barley agrimore 26.3 0.919 ## 3 barley supergain 22.5 0.771 ## 4 barley supersupp 25.6 1.06 ## 5 oats control 20.5 0.506 ## 6 oats agrimore 23.3 0.613 ## 7 oats supergain 19.7 0.349 ## 8 oats supersupp 21.9 0.413 ## 9 wheat control 17.4 0.460 ## 10 wheat agrimore 19.6 0.710 ## 11 wheat supergain 17.0 0.485 ## 12 wheat supersupp 19.7 0.475 Replot the data with standard error bars: ggplot(sumMoo, aes(x=supplement, y=meanGrow, colour = diet, #adds color by group group = diet)) + # geom_point() + geom_line() + #connects the dots in each dataset geom_errorbar(aes(ymin = meanGrow - seGrow, ymax = meanGrow + seGrow), width = 0.1) + theme_bw() 6.2 Analysis of Covariance (ANCOVA) this linear model is unique in that it combines a categorical explanatory variable with a continuous explanatory variable Read in data: urlfile06b=&quot;https://raw.githubusercontent.com/apicellap/data/main/limpet.csv&quot; limp&lt;-read.csv(url(urlfile06b)) glimpse(limp) ## Rows: 24 ## Columns: 3 ## $ DENSITY &lt;int&gt; 8, 8, 8, 8, 8, 8, 15, 15, 15, 15, 15, 15, 30, 30, 30, 30, 30, … ## $ SEASON &lt;chr&gt; &quot;spring&quot;, &quot;spring&quot;, &quot;spring&quot;, &quot;summer&quot;, &quot;summer&quot;, &quot;summer&quot;, &quot;s… ## $ EGGS &lt;dbl&gt; 2.875, 2.625, 1.750, 2.125, 1.500, 1.875, 2.600, 1.866, 2.066,… this dataset involves egg production by limpets there are 4 density conditions in two production seasons the response (y) variable is egg production the independent (x) variables include: density (continuous) season (categorical) this is a study of density dependent reproduction Does density dependence of egg production differ between the spring and summer seasons? Coerce variable into a factor: limp$SEASON &lt;-as.factor(limp$SEASON) is.factor(limp$SEASON) #allows you to check that the coercion worked ## [1] TRUE Start by plotting the data: ggplot(limp, aes(x = DENSITY, y = EGGS, colour=SEASON)) + #colour - gives color to the two levels of the categorical variable, Grazing geom_point() + scale_color_manual(values = c(spring = &quot;green&quot;, summer = &quot;red&quot;)) + xlab(&quot;Limpet Density&quot;) + ylab(&quot;Eggs produced&quot;) + theme_bw() Output observations: As limpet density increases, there seems to be a decrease in egg production There appears to be a seasonal difference in egg production in which yields are better in spring than in summer mathematically, the intercept (the value of egg production) at a density of 0 is higher in spring Thinking about this plot in terms of regression: y is egg production x is density b is where the line crosses the y axis (egg production when density = 0) m is the slope of the egg production density relationship - Change in egg production per unit change in density - strength of density dependence Verbal, mathematical, R, and graphical interpretations of various hypotheses related to the ANCOVA that translate into specific linear models: In the above table, neither A or B reflect the data in the limpet density scatterplot E also does not do the data justice because the slopes are approximately parallel C and D are possible D is the best explanation of the data The difference between E and D is the presence (E) or absence (D) of an interaction term that specifies that the slopes are different An interaction would look like - “The effect on density on egg production depends on the season” The effect of density on egg production = slope value Depends on the season = values may differ depending on the season 6.2.1 Constructing the ANCOVA limp.mod &lt;- lm(EGGS ~ DENSITY * SEASON, #expression says we want model to analyze a (main) effect of DENSITY, #a (main) effect of SEASON, and the potential for the effect of DENSITY #to depend on SEASON (interaction ) data = limp) limp.mod ## ## Call: ## lm(formula = EGGS ~ DENSITY * SEASON, data = limp) ## ## Coefficients: ## (Intercept) DENSITY SEASONsummer ## 2.664166 -0.033650 -0.812282 ## DENSITY:SEASONsummer ## 0.003114 Null hypothesis: the interaction term is not significant That is to say - there are no differences in the slopes for each season There is no extra variation explained by fitting different slopes Fetch the coefficients (intercepts and slope estimates), the residuals, the fitted values, etc.: names(limp.mod) ## [1] &quot;coefficients&quot; &quot;residuals&quot; &quot;effects&quot; &quot;rank&quot; ## [5] &quot;fitted.values&quot; &quot;assign&quot; &quot;qr&quot; &quot;df.residual&quot; ## [9] &quot;contrasts&quot; &quot;xlevels&quot; &quot;call&quot; &quot;terms&quot; ## [13] &quot;model&quot; Check assumptions: autoplot(limp.mod,smooth.colour = NA) The assumptions are met according to the above figures Generate an anova table: anova(limp.mod) ## Analysis of Variance Table ## ## Response: EGGS ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## DENSITY 1 5.0241 5.0241 30.1971 2.226e-05 *** ## SEASON 1 3.2502 3.2502 19.5350 0.0002637 *** ## DENSITY:SEASON 1 0.0118 0.0118 0.0711 0.7925333 ## Residuals 20 3.3275 0.1664 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 output: big F values/small p values - DENSITY and SEASON - effects are significant/unlikely to be the result of chance effects of DENSITY and SEASON are additive small F value/big p value - DENSITY:SEASON - no interaction Generate a summary table: summary(limp.mod) ## ## Call: ## lm(formula = EGGS ~ DENSITY * SEASON, data = limp) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.65468 -0.25021 -0.03318 0.28335 0.57532 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.664166 0.234118 11.380 3.45e-10 *** ## DENSITY -0.033650 0.008259 -4.074 0.000591 *** ## SEASONsummer -0.812282 0.331092 -2.453 0.023450 * ## DENSITY:SEASONsummer 0.003114 0.011680 0.267 0.792533 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.4079 on 20 degrees of freedom ## Multiple R-squared: 0.7135, Adjusted R-squared: 0.6705 ## F-statistic: 16.6 on 3 and 20 DF, p-value: 1.186e-05 Upper portion of the table: R lists things alphanumerically (spring would be listed before summer) SEASONsummer is the difference between the spring and summer y intercepts My rough calculations estimated this to be -0.756, which is very close to their -0.81 DENSITY:SEASONsummer is the difference between slopes for spring and summer In other words, it is the change in the rate of density dependence that arises from shifting from spring to summer In the summmer, the density dependence was only 0.003 units (egg prod/density) lower than in spring which was found not to be statistically significant Summer reduces egg production compared with spring, on average, by 0.812 eggs - significant Bottom of coefficients table: Adjusted R-squared (0.6705) - means that the model we have fitted explains 67% of the variation in egg production Low p value - the model has a signficant fit to the data p &lt; 0.001 Large F statistic 6.2.2 Putting the lines onto the figure We need to predict some y values using a sensible range of x values We can use the model eggs(spring) = 2.66 - 0.033 x DENSITY to accomplish this coef(limp.mod) ## (Intercept) DENSITY SEASONsummer ## 2.664166462 -0.033649651 -0.812282493 ## DENSITY:SEASONsummer ## 0.003113571 Build a factorial representation of variables and output a grid of numbers: expand.grid(FIRST = c(&quot;A&quot;,&quot;B&quot;), SECOND = c(1,2)) ## FIRST SECOND ## 1 A 1 ## 2 B 1 ## 3 A 2 ## 4 B 2 Predict y values at all provided x values: predict(limp.mod) ## 1 2 3 4 5 6 7 8 ## 2.3949692 2.3949692 2.3949692 1.6075953 1.6075953 1.6075953 2.1594217 2.1594217 ## 9 10 11 12 13 14 15 16 ## 2.1594217 1.3938428 1.3938428 1.3938428 1.6546769 1.6546769 1.6546769 0.9358016 ## 17 18 19 20 21 22 23 24 ## 0.9358016 0.9358016 1.1499321 1.1499321 1.1499321 0.4777604 0.4777604 0.4777604 24 values returned because, 2 seasons x 4 density treatments x 3 replicates = 24 To generate a new set of x values we could use: new.x &lt;- expand.grid(DENSITY = seq(from =8, to=45, #the new x values will be between 8 and 45 length.out = 10), #generates 10 new x values for each parameter SEASON = levels(limp$SEASON)) #sets the parameter as season. head(new.x) ## DENSITY SEASON ## 1 8.00000 spring ## 2 12.11111 spring ## 3 16.22222 spring ## 4 20.33333 spring ## 5 24.44444 spring ## 6 28.55556 spring We can now embed these ‘new.x’ x values into the predict() function: new.y &lt;- predict(limp.mod, newdata = new.x, #instructs R to use the new x values interval = &#39;confidence&#39;) #for confidence interval around each y value (fit) head(new.y) ## fit lwr upr ## 1 2.394969 2.019285 2.770654 ## 2 2.256632 1.931230 2.582034 ## 3 2.118294 1.834274 2.402315 ## 4 1.979957 1.724062 2.235852 ## 5 1.841619 1.595998 2.087241 ## 6 1.703282 1.447918 1.958646 Now for some housekeeping: addThese &lt;- data.frame(new.x, new.y) #combines the two dfs into a single one addThese &lt;- rename(addThese, EGGS = fit) #changes the column name &quot;fit&quot; to EGGS head(addThese) ## DENSITY SEASON EGGS lwr upr ## 1 8.00000 spring 2.394969 2.019285 2.770654 ## 2 12.11111 spring 2.256632 1.931230 2.582034 ## 3 16.22222 spring 2.118294 1.834274 2.402315 ## 4 20.33333 spring 1.979957 1.724062 2.235852 ## 5 24.44444 spring 1.841619 1.595998 2.087241 ## 6 28.55556 spring 1.703282 1.447918 1.958646 Plot the data: ggplot(limp, aes(x=DENSITY, y = EGGS, colour = SEASON)) + geom_point(size = 2.5) + #specifies the size of the point geom_smooth( #adds the regression line and the CI around the fitted line; #assumes that x and y are the same as already specified in ggplot data = addThese, aes(ymin = lwr, ymax = upr, #confidence interval fill = SEASON), #different fill depending on the SEASON variable stat = &#39;identity&#39;) + #tells geom_smooth to use what is in the dataframe and not calculate anything else scale_colour_manual(values = c(spring=&quot;green&quot;, summer = &quot;red&quot;)) + #color for the regression line scale_fill_manual(values = c(spring=&quot;green&quot;, summer = &quot;red&quot;)) + #color for the confidence interval area theme_bw() "],["getting-started-with-generalized-linear-models.html", "Chapter 7 Getting Started with Generalized Linear Models 7.1 Counts and rates - Poisson GLMs 7.2 Doing it wrong 7.3 Doing it right - the Poisson GLM 7.4 When a Poisson GLM isn’t good for counts", " Chapter 7 Getting Started with Generalized Linear Models Response variables in linear model-based analyses have several common features including that: We assume they are continuous variables that can take negative and positive values and can be fractions We also assume that these data are bounded (but in practice, sometimes they are not) The models assume normally distributed residuals The model also assumes a constant mean-variance relationship Often there are response variables in which the normality assumption is violated Examples of data that violate assumptions: Integer valued data that is bounded and cannot be fractioned or negative (i.e. the number of people) Binary data (i.e. presence/absence data) Historically for these types of data, transformations are employed such as log10 transformations for counts or arcsin(sqrt()) transformations for proportion data This is where the generalized linear model (GLM) may be utilized 7.0.1 Counts and Proportions and the GLM Count data: Are bounded by 0 and infinity Violate the normality assumption Do not have a constant mean-variance relationship Example: How the number of offspring a female may produce over her lifetime relates to body size How a rate of occurence of events (births) depends on other variables (body size) Proportion data - data are often binary flowering data animal death species sex ratios These occurences are often related to 1+ explanatory variables These data are binomial 7.0.2 Key terms of GLM models Family The probability distribution that is assumed to describe the response variable In other words, it’s a mathematical statement of how likely events are Poisson and binomial distributions are examples of families Linear predictor This is an equation that describes how the different predictor variables (explanatory variables) affect the expected value of the response variable Link function Describes the mathematical relationship between the expected value of the response variable and the linear predictor, linking, the two aspects of the GLM glm() will be used to create generalized linear models 7.1 Counts and rates - Poisson GLMs 7.1.1 Counting Sheep - the data and question In this dataset, the response variable is a count variable We aim to understand how the rate of occurence of events depends on 1 or more explanatory variables Dataset backstory: It’s about a specific population of sheep These sheep are on an island off the west coast of Scotland They are unmanaged/feral population of Soay sheep There has been a lot of scientific interest on this population to study how the population is evolving Several studies have been done on female (ewe) fitness One way to measure fitness is to count the total number of offspring born to a female during her life - ‘lifetime reproductive success’ response variable = counts of offspring, which are poorly approximated by a normal distribution question: Does lifetime reproductive success increase with ewe bodymass? Do larger ewes produce more offspring? If so, then is there a heritable difference in body mass - is there selection pressure on this trait? install.packages(&quot;ggplot2&quot;, repos = &quot;https://cran.us.r-project.org&quot;) install.packages(&quot;dplyr&quot;, repos = &quot;https://cran.us.r-project.org&quot;) install.packages(&quot;ggfortify&quot;, repos = &quot;https://cran.us.r-project.org&quot;) library(ggplot2) library(dplyr) library(ggfortify) urlfile07a=&quot;https://raw.githubusercontent.com/apicellap/data/main/SoaySheepFitness.csv&quot; soay&lt;-read.csv(url(urlfile07a)) str(soay) ## &#39;data.frame&#39;: 50 obs. of 2 variables: ## $ fitness : int 4 3 2 14 5 2 2 5 8 4 ... ## $ body.size: num 6.37 7.18 6.16 8.6 7.33 ... Visualize the data: ggplot(soay, aes(x = body.size, y = fitness)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + #applies linear regression (blue) line geom_smooth(span = 1, #determines how wiggly the curve is colour = &quot;red&quot;, se = FALSE) + #applies a non-linear and more flexible statistical model xlab(&quot;Body mass (kg)&quot;) + ylab(&quot;Lifetime fitness&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; Based on the loose fitting models, there is a strong positive relationship between fitness and body size in which larger ewes have more offspring over time Larger ewes have more resources to allocate to reproduction The red line clearly captures the relationship more effectively than the blue one The upward trending of the red line is an issue of non-linearity There are also subtler issues with the data To really understand these problems, the authors want to analyze the data the incorrect way and then right the wrongs with the correct analysis after 7.2 Doing it wrong Create an inappropriate model using the linear model approach: soay.mod &lt;- lm(fitness ~ body.size, data = soay) autoplot(soay.mod, smooth.colour = NA) Almost all of our assumptions are violated 7.2.1 Doing it wrong: diagnosing the problems Residuals vs fitted values (upper left): Suggests that the systematic part of the model is inappropriate because there is a pattern of the datapoints In fact, the U-shape of the data indicate that the straight-line model fails to account for the curvature in the relationship between our two variables Fitness is underestimated at small body sizes Fitness is overestimated at larger body sizes Normal Q-Q plot (upper right): Most of the points should be on the dashed diagnonal line and they are not Many of the points in positive territory are above the line Many of the points in negative territory are below the line We see this pattern because the distribution of residuals is not symmetrical In this case, it is skewed to the right Visualize rightward skew through a histogram: ggplot(soay, aes(x=body.size))+ geom_histogram(bins=20) Scale location plot (bottom left): There is a clear pattern that shows a positive relationship between the absolute size of the residuals and the fitted values This reflects the way the fitness values are scattered around the red line: there is more vertical spread in the data at larger fitness values Based on this, we can say that there is a positive mean-variance relationship in the data In other words, large predicted fitness values are associated with more variation in the residuals This is typical of count data Residuals-leverage plot (bottom right): This plot is not too bad There are no extreme standardized residual values No obvious outliers None of the observations are having an outsized effect on the model Overall, the normal linear regression is not doing a good job of describing the data 7.2.2 The Poisson distribution - a solution The normality assumption built into the linear regression model is not appropriate for these data - Properties of a normal distribution: Concerns continuous variables (can assume fractional values) Count data are discete (i.e. a ewe can produce 0,1,2, etc. lambs) Normal distribution allows for negative values Count data must be positive (i.e. ewe cannot have -2 offspring) Symmetry. Count data often are asymmetrical Poisson distribution is a good starting point for the analysis of certain forms of count data A visual for understanding the Poisson distribution: The above figure displays three Poisson distributions and each has a different mean The x-axis has a range of different possible values and the y axis has the probability of each value Reasons why the poisson distribution is a good model for the Soay data: Only discete counts are possible Data are bounded at 0 Very large counts are possible and in the scope of the model; however, their occurence is unlikely Variance of the distribution increases as the mean of distribution is increased This corresponds to the widening of the base the distribution with a higher mean The poisson distribution is best equipped for unbounded count data What unbounded here refers to is just that there is no uppper limit to the values that the count variable may take There are obvious biological constraints, but we don’t actually know the limit 7.3 Doing it right - the Poisson GLM 7.3.1 Anatomy of a GLM GLM is comprised of three key terms: family, linear predictor, the link function family: This is the error aspect of the GLM Determines what kind of distribution is used to describe the response variable. Options: Poisson distribution Binomial distribution Gamma distribution (positive valued, continuous variables) Other exotic versions Each option is appropriate for specific types of data linear predictor: Every time a model is built with the lm() function, some data and an R formula must be supplied to define the model The R formula defines the linear predictor Soay example: inappropriate regression with model: lm(fitness ~ body.size, data = soay) This tells R to build a model for the predicted fitness given an intercept and body size slope term: Predicted fitness = Intercept + Slope x Body size The linear predictor is ‘Intercept + Slope x Body size’ so it’s basically just the model Coefficients shown by summary(soay.mod) are just the intercepts and slopes of th linear predictor The link function: A model that cannot make impossible predictions is preferred You can plug numbers into the formul that will produce illogical outputs such that: A negative number of offspring in this sheep example With the GLM, instead of trying to model the predicted values of the response variable directly, we model a mathematical transformation of the prediction The transformation is the link function Using a Poisson GLM to model the fitness vs body mass relationship, the model for the predicted fitness would look like this: Log[Predicted fitness] = Intercept + Slope x Body size This is a natural log in this case The link function in a standard Poisson GLM is always natural log Instead of the linear predictor describing fitness directly, it relates the natural logarithm of predicted fitness to body size This must be positive, but the log transformed value can take any value Solve for predicted fitness to get: Predicted fitness = e^{Intercept + Slope x Body size} A Poisson GLM for the Soays sheep implies an exponential relationship between fitness and body mass linear model does not mean a linear relationship The link function allows for the estimation of the paramenteres of a linear predictor that is apporpriate for the data This is accomplished by transforming the response ‘scale’ to the scale of the linear predictor, which in this case a natural log scale, which is defined by the link function This is can be visualized in the following figure: Figure: Count data is bounded by 0 and positive infinity Predicted values must be greater than 0 to be valid for count data But, to do effective statistics, we need to operate on a scale that is unbounded Using the link function accomplishes this It moves us from the positive numbers (predicted average counts) scale to the whole number line (the linear predictor) scale 7.3.2 Doing it right - actually fitting the model Build the GLM: soay.glm &lt;- glm(fitness ~ body.size, data = soay, family = poisson( link = log)) #this line is unnecessary because the log link function is the default for poisson glm 7.3.3 Doing it right - the diagnostics autoplot(soay.glm, smooth.colour = NA) Overall, the diagnostics look much better Residuals vs fitted values (upper left): The systematic aspect of the model is fairly appropriate There is no clear pattern in the relationship apart from a very slight upward trend Not enough to be concerned with though Normal Q-Q plot (upper right): There is some departure from the dashed diagnonal line, but a perfect plot is not expected Confirms that the distributional assumptions are okay Scale location plot (bottom left): There is a slight positve relationship between the size of the residuals and the fitted values, but not too much of a concern Residuals-leverage plot (bottom right): Shows no evidence that there are outliers or points having an outsized effect on the model Keep in mind that when R does diagnostics for a GLM, it uses standardized deviance residuals This is a transformed version of raw residuals that make transformed residuals normally distributed if the GLM family that is being used is appropriate This means that if the chosen family is appropriate for the data, then the diagnostics should show that the residuals are normally distributed And the diagnostic plots can be evaluated in the same way as for a linear model because the tests are doing all the same jobs 7.3.4 Doing it right - anova() and summary() Thus far it looks that fitnesss is positively related to body mass The next step is to test hypotheses Create Analysis of Deviance table for the GLM: anova(soay.glm) ## Analysis of Deviance Table ## ## Model: poisson, link: log ## ## Response: fitness ## ## Terms added sequentially (first to last) ## ## ## Df Deviance Resid. Df Resid. Dev ## NULL 49 85.081 ## body.size 1 37.041 48 48.040 “Deviance” here is closely related to the idea of likelihood, which is a general tool for doing statistics Short explanation of likelihood: provides us with a measure of how probable the data would be if they had really been produced by that model Using likelihood, you can find a set of best-fitting model coefficients by picking values that maximize likelihood Also, sum of squares and mean squares allow for the comparison of different models when normality is assumed Likelihood (and deviance) do the same thing for GLMs no p values in this table Total deviance for NULL (fitness) is 85.081 and body.size explains 37.041 units of the deviance - So bodysize accounts for almost half of the deviance. This is very large. There is no p value because R wants the type of test to calculate it to be specified p values in the GLM involve a \\(\\chi\\)\\(^{2}\\) distribution rather than an F-distribution (this does not mean that a \\(\\chi\\)\\(^{2}\\) test is to be performed) Specifiy the type of test: anova(soay.glm, test = &quot;Chisq&quot;) ## Analysis of Deviance Table ## ## Model: poisson, link: log ## ## Response: fitness ## ## Terms added sequentially (first to last) ## ## ## Df Deviance Resid. Df Resid. Dev Pr(&gt;Chi) ## NULL 49 85.081 ## body.size 1 37.041 48 48.040 1.157e-09 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The test statistic has a value of 37.041 The corresponding p value is very small This is unsurprising given the strong relationship visualized in the scatterplot This is a likelihood ratio test This means that fitness does vary positively with body size and there is selection pressure on higher body mass Find out more about the model: summary(soay.glm) ## ## Call: ## glm(formula = fitness ~ body.size, family = poisson(link = log), ## data = soay) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.7634 -0.6275 0.1142 0.5370 1.9578 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -2.42203 0.69432 -3.488 0.000486 *** ## body.size 0.54087 0.09316 5.806 6.41e-09 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 85.081 on 49 degrees of freedom ## Residual deviance: 48.040 on 48 degrees of freedom ## AIC: 210.85 ## ## Number of Fisher Scoring iterations: 4 Table output: First chunk is self explanatory Second chunk - “useless” summary of specially scaled (deviance) residuals Third chunk - coefficients model is a line so there are only two coefficients - the intercept and a slope Each coefficient has a standard error to tell us how precise it is and a z-value to help us determine if the estimate is significantly different from 0 and the associated p value Fourth chunk - dispersion parameter - more on this later Fifth chunk - summaries of the null deviance, residual deviance, and their dfs null deviance - measure of all the ‘variation’ in the data residual deviance - measure of what is left over after fitting the model The bigger the difference in these two values, the more variation is explained by the model AIC = Akaike information criterion (not analyzed in this text) Number of Fisher Scoring iterations - not important Upon looking at the original scatterplot that helped visualize the data and this summary table, it might be confusing to see that the intercept has a negative value But remember that the link function is being used to predict the natural logarithm of lifetime fitness, not actual fitness We will revisit the overdispersion concept later in the chapter 7.3.5 Making a beautiful graph Need to generate a set of new x values Generate new x values: min.size = min(soay$body.size) max.size = max(soay$body.size) new.x = expand.grid(body.size = seq(min.size, max.size, #the new x values will be between these two parameters length = 1000)) #R generates 1000 values between Now the predict() function can be used: predict() is given three arguments: the GLM model, a value for newdata, and a request for standard errors Side note: predict() for glm does not have a confidence interval argument, so they have to be manually rendered into the dataframe new.y &lt;- predict(soay.glm, newdata = new.x, #instructs R to use the new x values se.fit = TRUE) #provides standard errors.predict() for glm() doesn&#39;t have an interval = confidence argument new.y = data.frame(new.y) #converts new.y into a dataframe head(new.y) ## fit se.fit residual.scale ## 1 0.1661991 0.2541777 1 ## 2 0.1682619 0.2538348 1 ## 3 0.1703247 0.2534919 1 ## 4 0.1723874 0.2531491 1 ## 5 0.1744502 0.2528063 1 ## 6 0.1765130 0.2524635 1 Combine new x values with new y values: addThese = data.frame(new.x, new.y) names(addThese)[names(addThese) == &#39;fit&#39;] &lt;- &#39;fitness&#39; #this will match the original data head(addThese) ## body.size fitness se.fit residual.scale ## 1 4.785300 0.1661991 0.2541777 1 ## 2 4.789114 0.1682619 0.2538348 1 ## 3 4.792928 0.1703247 0.2534919 1 ## 4 4.796741 0.1723874 0.2531491 1 ## 5 4.800555 0.1744502 0.2528063 1 ## 6 4.804369 0.1765130 0.2524635 1 Calculate and include the confidence intervals in the dataframe: addThese = mutate(addThese, lwr = fitness -1.96 * se.fit, #this and the line below add the lower and upper bounds of the 95% CI upr = fitness +1.96 * se.fit) head(addThese) ## body.size fitness se.fit residual.scale lwr upr ## 1 4.785300 0.1661991 0.2541777 1 -0.3319891 0.6643874 ## 2 4.789114 0.1682619 0.2538348 1 -0.3292543 0.6657781 ## 3 4.792928 0.1703247 0.2534919 1 -0.3265195 0.6671689 ## 4 4.796741 0.1723874 0.2531491 1 -0.3237848 0.6685597 ## 5 4.800555 0.1744502 0.2528063 1 -0.3210501 0.6699506 ## 6 4.804369 0.1765130 0.2524635 1 -0.3183156 0.6713415 Visualize the data with our new model + predicted values: ggplot(soay, aes(x = body.size, y = fitness)) + geom_point(size = 3, alpha = 0.5) + geom_smooth(data = addThese, aes(ymin = lwr, ymax = upr), stat = &#39;identity&#39;) + theme_bw() it didn’t work: What happened is that the default scale for predict() is just the scale of the link function The link function uses a logarithmic scale This means that the predictions are the log of the expected fitness, but we want to plot the actual fitness values To fix this, the inverse log must be applied to any y-axis variables in addThese To inverse log, we just have to exponentiate them Recreate the y values: min.size = min(soay$body.size) max.size = max(soay$body.size) new.x = expand.grid(body.size = seq(min.size, max.size, length = 1000)) new.y &lt;- predict(soay.glm, newdata = new.x, se.fit = TRUE) new.y = data.frame(new.y) addThese = data.frame(new.x, new.y) head(addThese) ## body.size fit se.fit residual.scale ## 1 4.785300 0.1661991 0.2541777 1 ## 2 4.789114 0.1682619 0.2538348 1 ## 3 4.792928 0.1703247 0.2534919 1 ## 4 4.796741 0.1723874 0.2531491 1 ## 5 4.800555 0.1744502 0.2528063 1 ## 6 4.804369 0.1765130 0.2524635 1 Exponentiate the y values: addThese = mutate(addThese, fitness = exp(fit), lwr = exp(fit - 1.96 *se.fit), upr = exp(fit - 1.96*se.fit)) head(addThese) ## body.size fit se.fit residual.scale fitness lwr upr ## 1 4.785300 0.1661991 0.2541777 1 1.180808 0.7174951 0.7174951 ## 2 4.789114 0.1682619 0.2538348 1 1.183246 0.7194600 0.7194600 ## 3 4.792928 0.1703247 0.2534919 1 1.185690 0.7214303 0.7214303 ## 4 4.796741 0.1723874 0.2531491 1 1.188138 0.7234059 0.7234059 ## 5 4.800555 0.1744502 0.2528063 1 1.190591 0.7253869 0.7253869 ## 6 4.804369 0.1765130 0.2524635 1 1.193050 0.7273732 0.7273732 Revisulize the data: ggplot(soay, aes(x = body.size, y = fitness)) + geom_point(size = 3, alpha = 0.5) + geom_smooth(data = addThese, aes(ymin = lwr, ymax = upr), stat = &#39;identity&#39;) + theme_bw() 7.4 When a Poisson GLM isn’t good for counts The previous data set was the best case scenario for a GLM and the simplest use case Now we will work with more realistic data 7.4.1 Overdispersion Overdispersion means ‘extra variation’ GLMs make strong assumptions about the nature of variability in the data We have seen how the variance increases with the mean For a Poisson distribution, the variance is equal to the mean This assumption can only be true if you can account for every source of variation in the data (big assumption esp in biology) But Biology creates variation that cannot be measured or accounted for This is the source of the overdisperson problem From non-independence in the data Non-independence refers to the idea that some elements in the data are more similiar to one another than they are to other things i.e. cannabis plants in experiment are more similiar to each other than a random plant overdispersion sounds like BS but it can really mess up statistical output if it is ignored (i.e. p-values) and end up with false positives What to do about it? First: detect it. summary(soay.glm) ## ## Call: ## glm(formula = fitness ~ body.size, family = poisson(link = log), ## data = soay) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.7634 -0.6275 0.1142 0.5370 1.9578 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -2.42203 0.69432 -3.488 0.000486 *** ## body.size 0.54087 0.09316 5.806 6.41e-09 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 85.081 on 49 degrees of freedom ## Residual deviance: 48.040 on 48 degrees of freedom ## AIC: 210.85 ## ## Number of Fisher Scoring iterations: 4 If a GLM is working appropriately and there is no overdispersion, then residual deviance (48.040) and its degrees of freedom (48) should be equal Perform \\(\\frac{residual~deviance}{residual~degrees~of~freedom}\\) Then the output is a ‘dispersion index (DI)’ which should be approximately 1 If much bigger than 1, then the data are overdispersed As a rule of thumb, if \\(DI \\ge 2\\) If it is much less than 1, then the data are underdispersed (this is rare) Fortunately, the soay data have normal dispersion levels If data are overdispersed, then you can model the data differently A simple fix is to change the family in glm() from family = poisson to family = quasipoisson a quasi model works just like the glm but it also estimates the dispersion index in a much more clever way than we did Once this index value is known, then R can adjust the p-values accordingly Can also switch to a negative binomial family A negative binomial distribution is thought of as being more flexible than a Poisson distribution The variance increases with the mean, but in a less constrained way. The variance does not have to equal the mean soay.glm2 &lt;- glm(fitness ~ body.size, data = soay, family = quasipoisson( link = log)) summary(soay.glm2) ## ## Call: ## glm(formula = fitness ~ body.size, family = quasipoisson(link = log), ## data = soay) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.7634 -0.6275 0.1142 0.5370 1.9578 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -2.42203 0.65967 -3.672 0.000605 *** ## body.size 0.54087 0.08851 6.111 1.7e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for quasipoisson family taken to be 0.9026887) ## ## Null deviance: 85.081 on 49 degrees of freedom ## Residual deviance: 48.040 on 48 degrees of freedom ## AIC: NA ## ## Number of Fisher Scoring iterations: 4 The only difference between this summary statistics table and the original one for the soay glm is the p-values. They are based on a method that accounts for over/underdispersion One more thing. Need to tell R to take into account the estimated dispersion by: anova(soay.glm, test = &quot;F&quot;) #instead of Chisq ## Analysis of Deviance Table ## ## Model: poisson, link: log ## ## Response: fitness ## ## Terms added sequentially (first to last) ## ## ## Df Deviance Resid. Df Resid. Dev F Pr(&gt;F) ## NULL 49 85.081 ## body.size 1 37.041 48 48.040 37.04 1.157e-09 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 7.4.1.1 Negative binomials Use glm.nb() to build negative binomial models from the MASS package (base-R) There is little else new that I need to learn The default link function is the natural log, but can check ?glm.nb for alternatives Neither quasi or negative binomial models appropriately deal with overdispersion that is caused by non-independence This is the job of mixed models, which will not be discussed in this text 7.4.2 Zero inflation One specific source of overdispersion in Poisson-like count data is known as zero-inflation This occurs when there are too many zeroes relative to the expected number for whichever distribution is being used If count is zero-inflated, you can often spot it with a chart of raw counts (i.e. spike at 0 on a bar chart - often results from zero-inflation) Biological counts are often zero-inflated Occurs when binary results combine with a Poisson process 7.4.3 Transformations ain’t all bad Sometimes transformations are appropriate, sometimes not When in doubt, build the model and test the diagnostics If the diagnostics look okay, then it’s probably fine to use the model Advantages of using transformations: They work well when the data are far away from zero (no zeroes), but don’t span orders of magnitiude Can analyze “split-plot” designs fairly easily "],["chapter-8-pimping-your-plots.html", "Chapter 8 Chapter 8: Pimping Your Plots", " Chapter 8 Chapter 8: Pimping Your Plots install.packages(&quot;ggplot2&quot;, repos = &quot;https://cran.us.r-project.org&quot;) install.packages(&quot;dplyr&quot;, repos = &quot;https://cran.us.r-project.org&quot;) install.packages(&quot;gridExtra&quot;, repos = &quot;https://cran.us.r-project.org&quot;) library(ggplot2) library(dplyr) library(gridExtra) urlfile08a=&quot;https://raw.githubusercontent.com/apicellap/data/main/compensation.csv&quot; compensation&lt;-read.csv(url(urlfile08a)) head(compensation) ## Root Fruit Grazing ## 1 6.225 59.77 Ungrazed ## 2 6.487 60.98 Ungrazed ## 3 4.919 14.73 Ungrazed ## 4 5.130 19.28 Ungrazed ## 5 5.417 34.25 Ungrazed ## 6 5.359 35.53 Ungrazed Create base scatterplot: eg_scatter &lt;- ggplot(compensation, aes(x = Root, y = Fruit)) + geom_point() eg_scatter Create base boxplot: eg_box &lt;- ggplot(compensation, aes(x = Grazing, y = Fruit)) + geom_boxplot() eg_box Render blank background: eg_scatter + theme_bw() Arrange plots with gridExtra: grid.arrange( eg_scatter, eg_box, nrow =1) #specifies the arrangement Change axes’ bounds: eg_scatter + xlim(0,20) +ylim(0,140) Add text to plot: eg_scatter + annotate(&quot;text&quot;, x=c(6,8), #6 and 105 are the x,y coordinates for placing the label y=c(105,25), label = c(&quot;here&quot;,&quot;there&quot;)) Modify axis scales: eg_scatter + scale_x_continuous(limits = c(4,11), #bounds breaks = 4:11) #tick marks at 1 step between the bounds ggplot(compensation, aes(x = Root, y = Fruit, color = Grazing)) + #need to add the color to aes for scale_color_manual() to function geom_point() + scale_color_manual(values = c(Grazed = &quot;brown&quot;, Ungrazed = &quot;green&quot;)) Transform the scale: eg_box + scale_y_continuous(breaks = seq(from = 10, to = 150, by = 20), #vector breakpoints that call for ticks trans = &quot;log10&quot;) #log transformation of y axis Modifying the theme: eg_scatter + theme( panel.background = element_rect(fill = NA, colour = &quot;black&quot;), #backgrounds is white panel.grid.minor = element_blank(), #no minor gridlines panel.grid.major = element_line(colour = &quot;lightblue&quot;) #gridlines are blue ) Modify elements of the x axis: eg_box + theme( axis.title.x = element_text(color = &quot;cornflowerblue&quot;, size = rel(2)), #relative increase above the default setting axis.text.x = element_text(angle = 45, #angle of x axis labels size = 13, vjust =0.5 ) #scoots labels down a bit; can accept values from 0-1 ) Modify axis labels (categorical variables): eg_box + scale_x_discrete(limits = c(&quot;Ungrazed&quot;, &quot;Grazed&quot;), #limits refer to variables in the dataframe labels = c(&quot;Control&quot;, &quot;Grazed&quot;)) #labels() corresponds to the limits and lets you change them without altering the df Modify legend: ggplot(compensation, aes(x = Root, y = Fruit, color = Grazing)) + geom_point() + theme(legend.key = element_rect(fill = NA)) #removes box around the legend "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
